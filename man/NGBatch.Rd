% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ng_expose.R
\name{NGBatch}
\alias{NGBatch}
\title{Neural Gas Batch Learning}
\usage{
NGBatch(
  X,
  W,
  lambda = 0.25 * nrow(W),
  lambda_decay = 0.9,
  lambda_schedule = NULL,
  tol_delBMU = 1,
  tol_delQE = 1,
  max_epochs = -1,
  XLabel = NULL,
  parallel = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{X}{a data matrix, one observation per row.}

\item{W}{the number of NG prototypes and their initialization, see \code{\link{NGWInitialization}}.}

\item{lambda}{the starting value of the neighborhood factor for cooperative learning. 
Typical values are 25-50\% of the number of prototypes in the network. Optional, default = 0.25*nrow(W).}

\item{lambda_decay}{a multiplicative decay factor applied to lambda after every learning epoch. Ex.: lambda(t) = lambda(t-1)*decay.
Optional, default = 0.9. Must be < 1 to ensure convergence.  Higher values require longer training time, but typically give better quantization.}

\item{lambda_schedule}{a named vector to set a custom annealing schedule for lambda instead of the multiplicative decay controlled by \code{lambda} and \code{lambda_decay}. 
\code{names(lambda_schedule)} should be integers defining the epoch \strong{through} which the corresponding elements of the vector are applied. 
Optional; if given, this schedule over-rides any multiplicative annealing specified by \code{lambda} and \code{lambda_decay}.}

\item{tol_delBMU}{tolerance controlling convergence of the learning, see \code{\link{NGConvergence}}. 
Optional, default = 1.}

\item{tol_delQE}{tolerance controlling convergence of the learning, see \code{\link{NGConvergence}}. 
Optional, default = 1.}

\item{max_epochs}{tolerance controlling convergence of the learning, see \code{\link{NGConvergence}}. 
Optional, default = -1.}

\item{parallel}{whether to compute in parallel (recommended, default = TRUE).}

\item{verbose}{whether to print learning history to the console after each epoch. Default = TRUE.}

\item{Xlabel}{optionally, a vector of labels for the data in the rows of X.  These can be of any type (character, factor, numeric) but will be converted to integers internally. 
If given, Xlabel allows reporting additional quality measures during the learning process as described in \code{\link{NGLearnHist}}.}
}
\value{
a list with components: 
\describe{
\item{W}{the learned prototype matrix, one prototype per row}
\item{age}{the age of the network (number of epochs trained). age=max_epochs (if given), otherwise it records the number of epochs required to attain convergence according to the criteria in \code{tol_delBMU} and \code{tol_delQE.}}
\item{lambda_start}{the value of lambda at the beginning of learning (the value of initial lambda given)}
\item{lambda_end}{the value of lambda at the end of leanring}
\item{lambda_decay}{the supplied decay factor}
\item{lambda_schedule}{the schedule set in \code{lambda_schedule}, if given}
\item{tol_delBMU}{the supplied value of tol_delBMU}
\item{tol_delQE}{the supplied value of tol_delQE}
\item{max_epochs}{the supplied value of max_epochs}
\item{exec_time}{execution time, in minutes}
\item{LearnHist}{a data frame recording various learning histories, see \code{\link{NGLearnHist}}}
}
}
\description{
Neural Gas Batch Learning
}
\details{
Neural gas finds prototypes \code{W} which minimize the following cost function: 
\deqn{\sum_i \sum_j h_{ij} d(x_i, w_j)}{sum_i(sum_j( h_ij x d(x_i,w_j) ))}
where the neighborhood function 
\deqn{h_{ij} = exp(-k_{ij} / \lambda)}{h_ij = exp(-k_ij / lambda)}
and \eqn{k_{ij}}{k_ij} = the rank of \eqn{d(x_i, w_j)}, with respect to all other \eqn{d(x_i, w_k)}.    
Ranks are ascending, and by convention start at 0 (instead of 1). 

Batch learning updates the prototypes after presentation of all data to the network. 
The update rule is implemented according to the method of:
Cottrell, M., Hammer, B., Hasenfuss, A., & Villmann, T. (2006). \emph{Batch and median neural gas}
}
