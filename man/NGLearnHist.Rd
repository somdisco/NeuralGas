% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ng_expose.R
\name{NGLearnHist}
\alias{NGLearnHist}
\title{Learning History for NeuralGas Objects}
\description{
\strong{Note:} This is not a function, merely a description of the \code{LearnHist} data frame returned from the \code{NGBatch} and \code{NGOnline} functions.
}
\details{
At the end of each epoch the following monitoring measures are computed, reported, and stored in the \code{LearnHist} data frame 
returned in the output list. For online learning one 'Epoch' is equivalent to \code{N} learning iterations, where \code{N} is the 
number of training vectors:  \code{N = nrow(X)} where \code{X} is the matrix of training data.  

\describe{
\item{Epoch}{the epoch for which measures are reported}
\item{alpha}{the effective learning rate}
\item{lambda}{the effective neighborhood lambda}
\item{Cost}{the value of the NG cost function, divided by the number of data vectors \code{N}. 
The purpose of division by \code{N} is to put \code{Cost} on a similar scale to \code{MQE}.}
\item{MQE}{Mean Quantization Error of all data}
\item{NhbEff}{the effect of the current value of lambda on the network, calculated as \code{Cost} / \code{MQE}. 
\code{NhbEff} is always >= 1, with values = 1 indicating no neighborhood effect, which occurs as lambda -> 0.}
\item{delCost}{the relative absolute percent change in Cost, from epoch(t-1) to epoch(t), = 
abs(Cost(t) - Cost(t-1)) / Cost(t) * 100 }
\item{delQE}{the relative absolute percentage change in the quantization error for each data vector, 
from epoch(t-1) to epoch(t), averaged over all data.}
\item{delBMU}{the proportion of data whose BMU has changed from epoch(t-1) to epoch(t)}
\item{RFEnt}{Normalized Shannon Entropy of the VQ mapping at epoch(t)}
}

If data labels are given (by supplying a value for \code{Xlabel}) we can use the learned VQ mapping to project these labels onto 
the prototypes. Each prototype's label (denoted RFL = Receptive Field Label) is decided by plurality vote of the labels of the data in its receptive field (RF). 
In the presence of labels, additional measures are computed, reported and stored in \code{LearnHist}: 
\describe{
\item{RFLPur}{Average of the individual Purity scores of each receptive field. 
The Purity score of each RF = \#(label(x)==RFL)/\#(RF), for all data x in the RF. This measures how much label confusion 
exists in the RF; ideally, if the labels indicate well separated classes / clusters we would have Purity=1. The value reported in 
RFLPur is the average Purity score of each RF, weighted by the RF's size (number of data vectors mapped to it). 
Purity -> 0 as intra-RF label confusion increases.}
\item{RFLUnq}{The number of unique RFLabels. This is a helpful measure to determine how well the prototypes represent X 
in situations with unbalanced class size, particularly when there are rare classes (of small size). Ideally, all unique labels 
found in Xlabel should be represented by some (set of) prototype(s).}
}
\item{RFLHell}{The Hellinger Distance between the empirical categorical distributions of Xlabels and RFLabels. 
RFLHell=0 means the distributions perfectly align; any value > 0 indicates disagreement. 
For example, assume the data are labeled one of A, B, or C, and (empirically, according to Xlabel), 
pX(A) = 0.20, pX(B) = 0.30 and pX(C) = 0.50. If the VQ has produced the mapping pRF(A) = 0.20, pRF(B) = 0.30 and pRF(C) = 0.50, then 
the distributions of data and RF labels perfectly agree, and RFLHell = 0. 
}
}
